
@article{Breiman.2001,
 abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
 author = {Breiman, Leo},
 year = {2001},
 title = {Random Forests},
 url = {https://doi.org/10.1023/A:1010933404324},
 pages = {5--32},
 volume = {45},
 number = {1},
 issn = {1573-0565},
 journal = {Machine Learning},
 doi = {10.1023/A:1010933404324}
}


@book{Breiman.post2005cop.1984,
 author = {Breiman, Leo},
 year = {post 2005], cop. 1984},
 title = {Classification and regression trees},
 address = {Boca Raton},
 publisher = {{Chapman {\&} Hall/CRC}},
 isbn = {978-0-412-04841-8}
}


@article{Burges.1998,
 author = {Burges, Christopher J.C.},
 year = {1998},
 title = {A Tutorial on Support Vector Machines for Pattern Recognition},
 url = {https://doi.org/10.1023/A:1009715923555},
 pages = {121--167{\dq},issn={\dq}1573-756X},
 volume = {2},
 number = {2},
 journal = {Data Mining and Knowledge Discovery},
 doi = {10.1023/A:1009715923555}
}

% This file was created with Citavi 5.3.1.0

@misc{Marr.2016,
 editor = {Marr, Bernard},
 year = {2016},
 title = {A Short History of Machine Learning},
 url = {https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/\#2df484f415e7}
}

@misc{techdocu,
 editor = {Texas Instrument's},
 year = {2015},
 title = {Multi-Standard CC2650 SensorTag Design Guide},
 url = {http://www.ti.com/lit/ug/tidu862/tidu862.pdf}
}


@article{Cox.1958,
 author = {Cox, David R.},
 year = {1958},
 title = {The regression analysis of binary sequences},
 pages = {215--242},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)}
}


@article{Friedman.2000,
 author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert and others},
 year = {2000},
 title = {Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)},
 pages = {337--407},
 volume = {28},
 number = {2},
 journal = {The annals of statistics}
}


@proceedings{IBMNewYork.2001,
 year = {2001},
 title = {IJCAI 2001 workshop on empirical methods in artificial intelligence},
 institution = {{IBM New York}}
}


@article{James.2013,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 year = {2013},
 title = {An introduction to statistical learning},
 volume = {112}
}


@inproceedings{Rish.2001,
 author = {Rish, Irina},
 title = {An empirical study of the naive Bayes classifier},
 keywords = {final thema:llda},
 pages = {41--46},
 volume = {3},
 booktitle = {IJCAI 2001 workshop on empirical methods in artificial intelligence},
 year = {2001}
}


@article{Russell.1995,
 author = {Russell, Stuart and Norvig, Peter and Intelligence, Artificial},
 year = {1995},
 title = {A modern approach},
 pages = {27},
 volume = {25},
 journal = {Artificial Intelligence. Prentice-Hall, Egnlewood Cliffs}
}


@book{Seni.2010,
 author = {Seni, Giovanni and Elder, John},
 year = {2010},
 title = {Ensemble methods in data mining: Improving accuracy through combining predictions /   Giovanni Seni, John Elder},
 address = {United States},
 publisher = {{Morgan {\&} Claypool}},
 isbn = {9781608452842},
 series = {Synthesis lectures on data mining and knowledge discovery,   2151-0067}
}


@article{gridsearch,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2346413},
 abstract = {A grid-search method of fitting segmented regression curves with unknown transition points is described and compared with a standard method. It is shown to be suitable for fitting a wider class of models than the standard method and to provide as a by-product a way of making reliable inferences about the abscissae of the transitions.},
 author = {P. M. Lerman},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {77-84},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Fitting Segmented Regression Models by Grid Search},
 volume = {29},
 year = {1980}
}

@article{RonKohavi.1995,
 author = {{Ron Kohavi}},
 year = {1995},
 title = {A study of~Cross-Validation and Bootstrp for Accuracy Estimation and Model Selection},
 url = {\url{https://pdfs.semanticscholar.org/0be0/d781305750b37acb35fa187febd8db67bfcc.pdf}},
 urldate = {08/20/2017}
}

@inproceedings{kfold,
 author = {Blum, Avrim and Kalai, Adam and Langford, John},
 title = {Beating the Hold-out: Bounds for K-fold and Progressive Cross-validation},
 booktitle = {Proceedings of the Twelfth Annual Conference on Computational Learning Theory},
 series = {COLT '99},
 year = {1999},
 isbn = {1-58113-167-4},
 location = {Santa Cruz, California, USA},
 pages = {203--208},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/307400.307439},
 doi = {10.1145/307400.307439},
 acmid = {307439},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@ARTICLE{evalmetrics, 
author={J. D. Rodriguez and A. Perez and J. A. Lozano}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Sensitivity Analysis of k-Fold Cross Validation in Prediction Error Estimation}, 
year={2010}, 
volume={32}, 
number={3}, 
pages={569-575}, 
keywords={Bayes methods;estimation theory;learning (artificial intelligence);pattern classification;probability;statistical analysis;classification error estimator;k-fold cross-validation;machine learning;naive Bayes method;nearest neighbor algorithm;prediction error estimation;probability distribution;sensitivity analysis;statistical properties;bias and variance;decomposition of the variance;error estimation;k-fold cross validation;prediction error;sources of sensitivity;supervised classification.}, 
doi={10.1109/TPAMI.2009.187}, 
ISSN={0162-8828}, 
month={March},}

@inproceedings{evalmetrics2,
 author = {Goutte, Cyril and Gaussier, Eric},
 title = {A Probabilistic Interpretation of Precision, Recall and F-score, with Implication for Evaluation},
 booktitle = {Proceedings of the 27th European Conference on Advances in Information Retrieval Research},
 series = {ECIR'05},
 year = {2005},
 isbn = {3-540-25295-9, 978-3-540-25295-5},
 location = {Santiago de Compostela, Spain},
 pages = {345--359},
 numpages = {15},
 url = {http://dx.doi.org/10.1007/978-3-540-31865-1_25},
 doi = {10.1007/978-3-540-31865-1_25},
 acmid = {2149993},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 

@misc{mck,
 editor = {McKinsey Global Institute},
 year = {2016},
 title = {The Age of Analytics: Competing In A Data-Driven World},
 url = {http://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/the-age-of-analytics-competing-in-a-data-driven-world}
}



